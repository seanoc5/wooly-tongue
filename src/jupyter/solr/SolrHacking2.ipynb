{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff1043e-c982-42d9-abed-1b8544b1516f",
   "metadata": {},
   "source": [
    "# Solr Hacking (2)\n",
    "\n",
    "From chatgpt: https://chatgpt.com/c/1cd2c89f-5a32-4ae1-a4f0-ca5aaf54a6a6"
   ]
  },
  {
   "cell_type": "code",
   "id": "727c0351-8fe0-495c-bc19-409ffbbc1888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T00:59:22.915070Z",
     "start_time": "2024-08-28T00:59:22.618731Z"
    }
   },
   "source": [
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import SentenceDetector, Tokenizer, BertSentenceEmbeddings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6b84b7ec-7638-4077-84e3-a76354429ab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T00:59:27.556012Z",
     "start_time": "2024-08-28T00:59:23.773554Z"
    }
   },
   "source": [
    "# Start SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with text\n",
    "data = [\n",
    "    (1, \"This is the first sentence.\"),\n",
    "    (2, \"Here is another sentence.\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"text\"])\n",
    "\n",
    "# Document Assembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Sentence Embeddings\n",
    "sentence_embeddings = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_768\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingLayer(\"mean\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    sentence_embeddings\n",
    "])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 20:59:25 WARN Utils: Your hostname, minti9 resolves to a loopback address: 127.0.1.1; using 192.168.1.101 instead (on interface enp4s0)\n",
      "24/08/27 20:59:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/27 20:59:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/27 20:59:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Document Assembler\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m document_assembler \u001B[38;5;241m=\u001B[39m \u001B[43mDocumentAssembler\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \\\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Sentence Detector\u001B[39;00m\n\u001B[1;32m     21\u001B[0m sentence_detector \u001B[38;5;241m=\u001B[39m SentenceDetector() \\\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCols([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \\\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/__init__.py:139\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/sparknlp/base/document_assembler.py:96\u001B[0m, in \u001B[0;36mDocumentAssembler.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;129m@keyword_only\u001B[39m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDocumentAssembler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mclassname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcom.johnsnowlabs.nlp.DocumentAssembler\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setDefault(outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m, cleanupMode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdisabled\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/__init__.py:139\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/sparknlp/internal/annotator_transformer.py:36\u001B[0m, in \u001B[0;36mAnnotatorTransformer.__init__\u001B[0;34m(self, classname)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetParams(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m_java_class_name \u001B[38;5;241m=\u001B[39m classname\n\u001B[0;32m---> 36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_java_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muid\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     84\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n\u001B[1;32m     85\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjava_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mjava_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ff7f3944bbded0d0",
   "metadata": {},
   "source": [
    "# Fit and transform the pipeline\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T00:59:31.404220Z",
     "start_time": "2024-08-28T00:59:31.391650Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Fit and transform the pipeline\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[38;5;241m.\u001B[39mfit(df)\n\u001B[1;32m      3\u001B[0m result \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(df)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# UDF to convert embeddings to array of floats\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "# UDF to convert embeddings to array of floats\n",
    "def extract_embeddings(embeddings):\n",
    "    return [float(x) for x in embeddings[0].embeddings]\n",
    "\n",
    "# Register UDF\n",
    "extract_embeddings_udf = udf(extract_embeddings, ArrayType(FloatType()))\n",
    "\n",
    "# Apply UDF to get the embeddings in the required format\n",
    "result_with_embeddings = result.withColumn(\n",
    "    \"sentence_embeddings\", extract_embeddings_udf(col(\"sentence_embeddings\"))\n",
    ")\n",
    "\n",
    "# Select relevant columns and rename\n",
    "final_df = result_with_embeddings.select(\n",
    "    col(\"sentence.result\").alias(\"sentence\"),\n",
    "    \"sentence_embeddings\"\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "final_df.show(truncate=False)\n",
    "\n",
    "# Saving to Solr\n",
    "final_df.write.format(\"solr\") \\\n",
    "    .option(\"zkhost\", \"your_zookeeper_host\") \\\n",
    "    .option(\"collection\", \"your_solr_collection_name\") \\\n",
    "    .save()"
   ],
   "id": "e2fb63fa-9cee-4e41-a4fa-d09f2c482e8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "72670ecf1d6b674"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wt2",
   "language": "python",
   "name": "wt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
