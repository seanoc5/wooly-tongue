{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjlwUPWugM37"
   },
   "source": [
    "# Use ChromaDocumentStore with Haystack\n",
    "\n",
    "https://docs.haystack.deepset.ai/docs/chromadocumentstore\n",
    "\n",
    "WORKING (chromadb2 env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "znSRD-hO2doM"
   },
   "outputs": [],
   "source": [
    "# Install the Chroma integration, Haystack will come as a dependency\n",
    "# !pip install -U chroma-haystack \"huggingface_hub>=0.22.0\"\n",
    "\n",
    "#!pip install chroma-haystack\n",
    "#!pip install torch torchaudio torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt_XhGXBgU-I"
   },
   "source": [
    "## Indexing Pipeline: preprocess, split and index documents\n",
    "In this section, we will index documents into a Chroma DB collection by building a Haystack indexing pipeline. Here, we are indexing documents from the [VIM User Manuel](https://vimhelp.org/) into the Haystack `ChromaDocumentStore`.\n",
    "\n",
    " We have the `.txt` files for these pages in the examples folder for the `ChromaDocumentStore`, so we are using the [`TextFileToDocument`](https://docs.haystack.deepset.ai/v2.0/docs/textfiletodocument) and [`DocumentWriter`](https://docs.haystack.deepset.ai/v2.0/docs/documentwriter) components to build this indexing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Torchy?? <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Torchy?? \u001B[1;3;31mTrue\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print  # https://rich.readthedocs.io/en/stable/markup.html#console-markup\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "print(f\"Torchy?? [bold red]{torch.cuda.is_available()}[/]\")\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'../data/test.txt'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'../data/.ipynb_checkpoints/test-checkpoint.txt'</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m[\u001B[0m\u001B[1;35mPosixPath\u001B[0m\u001B[1m(\u001B[0m\u001B[32m'../data/test.txt'\u001B[0m\u001B[1m)\u001B[0m, \u001B[1;35mPosixPath\u001B[0m\u001B[1m(\u001B[0m\u001B[32m'../data/.ipynb_checkpoints/test-checkpoint.txt'\u001B[0m\u001B[1m)\u001B[0m\u001B[1m]\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# file_paths = [\"data\" / Path(name) for name in os.listdir(\"data\")]\n",
    "from pathlib import Path\n",
    "data_dir = \"../data/text\"\n",
    "text_files = list(Path(data_dir).glob(\"**/*.txt\"))\n",
    "print(text_files)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "document_store = ChromaDocumentStore()\n",
    "\n",
    "# remove repeated substrings to get rid of headers/footers\n",
    "cleaner = DocumentCleaner(remove_repeated_substrings=True)\n",
    "# Since jina-v2 can handle 8192 tokens, 500 words seems like a safe chunk size\n",
    "splitter = DocumentSplitter(split_by=\"word\", split_length=100)\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=embedding_model)\n",
    "document_writer = DocumentWriter(document_store)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "indexing = Pipeline()\n",
    "indexing.add_component(\"converter\", TextFileToDocument())\n",
    "indexing.add_component(instance=cleaner, name=\"document_cleaner\")\n",
    "indexing.add_component(instance=splitter, name=\"document_splitter\")\n",
    "indexing.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "indexing.add_component(instance=document_writer, name=\"document_writer\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "indexing.connect(\"converter\", \"document_cleaner\")\n",
    "indexing.connect(\"document_cleaner\", \"document_splitter\")\n",
    "indexing.connect(\"document_splitter\", \"document_embedder\")\n",
    "indexing.connect(\"document_embedder\", \"document_writer\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "indexing.run({\"converter\": {\"sources\": text_files}})"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create pipeline components\n",
    "from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "retriever = ChromaQueryTextRetriever(document_store=document_store, top_k=3)\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=embedding_model)\n",
    "text_embedder.warm_up()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query_pipeline = Pipeline()\n",
    "# query_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "query_pipeline.add_component(\"retriever\", retriever)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test components...\n",
    "foo = retriever.run(query=\"test me\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(foo[\"documents\"])"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "q = \"How many languages are there?\"\n",
    "k = 5\n",
    "query_embedding = text_embedder.run(q)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# result = query_pipeline.run({\"text_embedder\": {\"text\": query}})\n",
    "result = query_pipeline.run({\"retriever\": {\"query\": q, \"top_k\": k}})\n",
    "# result = query_pipeline.run(query = query)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hl_list = [\n",
    "        r\"(?i)(?P<learn>ai|learn[a-z]*|lesson|ml|train[a-z]*|recogni[a-z]*|analy[a-z]*)\",\n",
    "        r\"(?i)(?P<machine>(machine|computer|bot|agent|system|recommenda|speech|voice)[a-z]*)\",\n",
    "        r\"(?i)(?P<model>embed[a-z]*|large|language|model[a-z]*|ml|llm|generat[a-z]*)\",\n",
    "        r\"(?i)(?P<orgs>firm[a-z]*|org[a-z]*|compan[a-z]*|instit[a-z]*)\",\n",
    "        r\"(?i)(?P<actions>(chat|search|build|generate|follow|create|use)[a-z]*)\",\n",
    "    ]\n",
    "\n",
    "class MyHighlighter(RegexHighlighter):\n",
    "    \"\"\"Apply style to anything that looks like an email.\"\"\"\n",
    "    base_style = \"example.\"\n",
    "    highlights = hl_list\n",
    "\n",
    "my_ml = MyHighlighter()\n",
    "\n",
    "theme = Theme(\n",
    "    {\"example.learn\": \"bold magenta\",\n",
    "     \"example.machine\": \"bold green\",\n",
    "     \"example.model\": \"bold blue\",\n",
    "     \"example.orgs\": \"bold yellow\",\n",
    "     \"example.actions\": \"bold red\",\n",
    "     })\n",
    "console.theme(theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(table_title, results, show_lines=True, ):\n",
    "    table = Table(title=table_title, show_lines=show_lines)\n",
    "    # table.add_column(\"id\", style=\"blue\", )  # no_wrap=True\n",
    "    table.add_column(\"Score\", style=\"blue\", )\n",
    "    table.add_column(\"text\")  # no_wrap=True\n",
    "    for d in results[\"retriever\"][\"documents\"]:\n",
    "        # print(f\"Result: \", d.meta, d.score)\n",
    "        table.add_row(\"{:.2f}\".format(d.score), d.content.strip(), )\n",
    "    console.print(table)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"Embedding model: [bold blue]{embedding_model}[/] \\nQuery: [bold blue]{q}[/]\"\n",
    "display_results(title, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44cRT55agw2e"
   },
   "source": [
    "## Query Pipeline: build retrieval-augmented generation (RAG) pipelines\n",
    "\n",
    "Once we have documents in the `ChromaDocumentStore`, we can use the accompanying Chroma retrievers to build a query pipeline. The query pipeline below is a simple retrieval-augmented generation (RAG) pipeline that uses Chroma's [query API](https://docs.trychroma.com/usage-guide#querying-a-collection).\n",
    "\n",
    "You can change the idnexing pipeline and query pipelines here for embedding search by using one of the [`Haystack Embedders`](https://docs.haystack.deepset.ai/v2.0/docs/embedders) accompanied by the  `ChromaEmbeddingRetriever`.\n",
    "\n",
    "\n",
    "In this example we are using:\n",
    "- The `HuggingFaceTGIGenerator` with the Mistral-7B-Instruct-v0.1. (You will need a Hugging Face token to use this model). You can repleace this with any of the other [`Generators`](https://docs.haystack.deepset.ai/v2.0/docs/generators)\n",
    "- The `PromptBuilder` which holds the prompt template. You can adjust this to a prompt of your choice\n",
    "- The `ChromaQueryRetriver` which expects a list of queries and retieves the `top_k` most relevant documents from your Chroma collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGGApIR3pllW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "hfat = \"HF_API_TOKEN\"\n",
    "if hfat in os.environ:\n",
    "    print(\"---------- Found hugging face token in environ, no need to prompt\")\n",
    "    hf_token = os.environ[\"HF_API_TOKEN\"]\n",
    "else:\n",
    "    print(\"++++++++++ Found hugging face token NOT in environ, need to prompt...\")\n",
    "    hf_token = getpass(\"Enter Hugging Face API key:\")\n",
    "    os.environ[\"HF_API_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQJTPYNreNV-"
   },
   "outputs": [],
   "source": [
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the query based on the provided context.\n",
    "If the context does not contain the answer, say 'Answer not found'.\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "  {{ doc.content }}\n",
    "{% endfor %}\n",
    "query: {{query}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder = PromptBuilder(template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQJTPYNreNV-"
   },
   "outputs": [],
   "source": [
    "from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever\n",
    "from haystack.components.generators import HuggingFaceTGIGenerator\n",
    "from haystack.utils import Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQJTPYNreNV-"
   },
   "outputs": [],
   "source": [
    "stoken = Secret.from_token(hf_token)\n",
    "# client = HuggingFaceTGIGenerator(model=\"mistralai/Mistral-7B-v0.1\", token=Secret.from_token(hf_token)\n",
    "client = HuggingFaceTGIGenerator(model=\"mistralai/Mistral-7B-Instruct-v0.2\", token=stoken)\n",
    "# HuggingFaceTGIGenerator(model=\"mistralai/Mistral-7B-v0.1\", token=Secret.from_token("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQJTPYNreNV-"
   },
   "outputs": [],
   "source": [
    "client.warm_up()\n",
    "response = client.run(\"What's Natural Language Processing?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQJTPYNreNV-"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFaceTGIGenerator(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "llm.warm_up()\n",
    "retriever = ChromaQueryTextRetriever(document_store)\n",
    "\n",
    "querying = Pipeline()\n",
    "querying.add_component(\"retriever\", retriever)\n",
    "querying.add_component(\"prompt_builder\", prompt_builder)\n",
    "querying.add_component(\"llm\", llm)\n",
    "\n",
    "querying.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "querying.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8jcmcdqrGu1"
   },
   "outputs": [],
   "source": [
    "query = \"What is the Revenue Capacity for jacksonville beach?\"\n",
    "query = \"How is annual enrollment assessed in Jax Beach schools?\"\n",
    "# NOTE / TODO: typo our outdated synax from example?? --> \"retriever\": {\"queries\": [query]... is wrong/broken\n",
    "results = querying.run({\"retriever\": {\"query\": query, \"top_k\": 3},\n",
    "                        \"prompt_builder\": {\"query\": query},\n",
    "                        \"llm\":{\"generation_kwargs\": {\"max_new_tokens\": 350}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa7f7EzjtBXw"
   },
   "outputs": [],
   "source": [
    "print(results[\"llm\"][\"replies\"][0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chromadb2",
   "language": "python",
   "name": "chromadb2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
