{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JULsCeQihR1"
   },
   "source": [
    "**Introduction**\n",
    "\n",
    "Most PDF to text parsers do not provide layout information. Often times, even the sentences are split with arbritrary CR/LFs making it very difficult to find paragraph boundaries. This poses various challenges in chunking and adding long running contextual information such as section header to the passages while indexing/vectorizing PDFs for LLM applications such as retrieval augmented generation (RAG).\n",
    "\n",
    "LayoutPDFReader solves this problem by parsing PDFs along with hierarchical layout information such as:\n",
    "\n",
    "Sections and subsections along with their levels.\n",
    "Paragraphs - combines lines.\n",
    "Links between sections and paragraphs.\n",
    "Tables along with the section the tables are found in.\n",
    "Lists and nested lists.\n",
    "With LayoutPDFReader, developers can find optimal chunks of text to vectorize, and a solution for limited context window sizes of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKDlbcX9M8_x"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZQry7z6iCtO"
   },
   "source": [
    "**Installation**\n",
    "\n",
    "Install the llmsherpa library."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BwpEalxgvlN",
    "outputId": "15bc51af-2035-4b43-d1b9-bf72357bafee",
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:13.056465Z",
     "start_time": "2024-06-06T13:49:13.054370Z"
    }
   },
   "source": [
    "# !pip install llmsherpa"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy8mKt1NislJ"
   },
   "source": [
    "The first step in using the LayoutPDFReader is to provide a url or file path to it and get back a document object."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wVCmWhxJiz9l",
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:17.016140Z",
     "start_time": "2024-06-06T13:49:13.107654Z"
    }
   },
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdfpdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "# pdf_url = \"https://www.census.gov/hfp/btos/downloads/CES-WP-24-16.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "\n",
    "# pdf_url = \"https://arxiv.org/pdf/2212.14024.pdf\"\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc = pdf_reader.read_pdf(pdf_url)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YiiW2-Ni9HY"
   },
   "source": [
    "**Install LlamaIndex**\n",
    "\n",
    "In the following examples, we will use LlamaIndex for simplicity. Install the library if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbVGDukQjdRd",
    "outputId": "da2e7c16-704f-470b-c703-3fd1aa4e1562",
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:17.023854Z",
     "start_time": "2024-06-06T13:49:17.019204Z"
    }
   },
   "source": [
    "# !pip install llama-index"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqGIVd6qn6eC"
   },
   "source": [
    "**Setup OpenAI**\n",
    "\n",
    "Make sure your API Key is inserted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3E73STaElAwN",
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:17.067534Z",
     "start_time": "2024-06-06T13:49:17.026538Z"
    }
   },
   "source": [
    "import openai\n",
    "import os\n",
    "from getpass import getpass\n",
    "# from openai import OpenAI\n",
    "\n",
    "if os.environ.get('OAI_KEY'):\n",
    "    print(f\"got OAI Key from environment var: 'OIA_KEY'\")\n",
    "    oai_key = os.environ['OAI_KEY']\n",
    "else:\n",
    "    oai_key = getpass(\"Enter an OPENAI key: \")\n",
    "    os.environ['OAI_KEY'] = oai_key\n",
    "oai_client = openai.OpenAI(api_key=oai_key)\n",
    "# oai_key"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got OAI Key from environment var: 'OIA_KEY'\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY09Id11jXzp"
   },
   "source": [
    "**Summarize a Section using prompts**\n",
    "\n",
    "LayoutPDFReader offers powerful ways to pick sections and subsections from a large document and use LLMs to extract insights from a section.\n",
    "\n",
    "The following code looks for the Fine-tuning section of the document:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "yOAON0CDj1mC",
    "outputId": "4aa514aa-5dbc-44de-b001-57e78dd7ff6f",
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:17.070525Z",
     "start_time": "2024-06-06T13:49:17.068231Z"
    }
   },
   "source": [
    "from IPython.core.display import display, HTML\n",
    "selected_section = None\n",
    "# find a section in the document by title\n",
    "for section in doc.sections():\n",
    "    if section.title == '3 Fine-tuning BART':\n",
    "        #if '3. AI Use Rates' in section.title:\n",
    "        print(f\"SELECTED Section title: {section.title} -- subsections:{len(section.sections())}\")\n",
    "        selected_section = section\n",
    "    else: \n",
    "        print(f\"Section title: {section.title}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section title: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
      "Section title: {mikelewis,yinhanliu,naman}@fb.com\n",
      "Section title: Abstract\n",
      "Section title: 1 Introduction\n",
      "Section title: B D A B C D E\n",
      "Section title: Bidirectional Encoder\n",
      "Section title: A _ C _ E\n",
      "Section title: <s> A B C D\n",
      "Section title: A B C D E\n",
      "Section title: 2 Model\n",
      "Section title: 2.1 Architecture\n",
      "Section title: 2.2 Pre-training BART\n",
      "Section title: Token Deletion Text Inﬁlling\n",
      "SELECTED Section title: 3 Fine-tuning BART -- subsections:4\n",
      "Section title: 3.1 Sequence Classiﬁcation Tasks\n",
      "Section title: 3.2 Token Classiﬁcation Tasks\n",
      "Section title: 3.3 Sequence Generation Tasks\n",
      "Section title: 3.4 Machine Translation\n",
      "Section title: 4 Comparing Pre-training Objectives\n",
      "Section title: 4.1 Comparison Objectives\n",
      "Section title: label\n",
      "Section title: A B C D E <s> A B C D E\n",
      "Section title: 4.2 Tasks\n",
      "Section title: 4.3 Results\n",
      "Section title: 5 Large-scale Pre-training Experiments\n",
      "Section title: 5.1 Experimental Setup\n",
      "Section title: 5.2 Discriminative Tasks\n",
      "Section title: 5.3 Generation Tasks\n",
      "Section title: R2 RL\n",
      "Section title: RO-EN\n",
      "Section title: 8 Conclusions\n",
      "Section title: References\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_705983/909753822.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:17.072363Z",
     "start_time": "2024-06-06T13:49:17.070909Z"
    }
   },
   "source": [
    "for s in selected_section.sections():\n",
    "    print(f\"Section: {section.title} \")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: References \n",
      "Section: References \n",
      "Section: References \n",
      "Section: References \n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "yOAON0CDj1mC",
    "outputId": "4aa514aa-5dbc-44de-b001-57e78dd7ff6f",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:17.074469Z",
     "start_time": "2024-06-06T13:49:17.072775Z"
    }
   },
   "source": [
    "# use include_children=True and recurse=True to fully expand the section.\n",
    "# include_children only returns at one sublevel of children whereas recurse goes through all the descendants\n",
    "#HTML(section.to_html(include_children=True, recurse=True))\n",
    "HTML(selected_section.to_html(include_children=True, recurse=True))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h3>3 Fine-tuning BART</h3><p>The representations produced by BART can be used in several ways for downstream applications.</p><h4>3.1 Sequence Classiﬁcation Tasks</h4><p>For sequence classiﬁcation tasks, the same input is fed into the encoder and decoder, and the ﬁnal hidden state of the ﬁnal decoder token is fed into new multi-class linear classiﬁer.\n",
       "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).</p><h4>3.2 Token Classiﬁcation Tasks</h4><p>For token classiﬁcation tasks, such as answer endpoint classiﬁcation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n",
       "This representation is used to classify the token.</p><h4>3.3 Sequence Generation Tasks</h4><p>Because BART has an autoregressive decoder, it can be directly ﬁne tuned for sequence generation tasks such as abstractive question answering and summarization.\n",
       "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n",
       "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.</p><h4>3.4 Machine Translation</h4><p>We also explore using BART to improve machine translation decoders for translating into English.\n",
       "Previous work Edunov et al.\n",
       "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n",
       "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).</p><p>More precisely, we replace BART’s encoder embedding layer with a new randomly initialized encoder.\n",
       "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n",
       "The new encoder can use a separate vocabulary from the original BART model.</p><p>We train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n",
       "In the ﬁrst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART’s encoder ﬁrst layer.\n",
       "In the second step, we train all model parameters for a small number of iterations.</p>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UnHxv2Ij-GO"
   },
   "source": [
    "Now, let's create a custom summary of this text using a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1-NOhTfkF3q",
    "outputId": "ad4fa612-1493-4c10-96e8-700587a8609f",
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:19.890140Z",
     "start_time": "2024-06-06T13:49:17.074864Z"
    }
   },
   "source": [
    "# from llama_index.llms import OpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "context = selected_section.to_html(include_children=True, recurse=True)\n",
    "question = \"list all the tasks discussed and one line about each task\"\n",
    "resp = OpenAI().complete(f\"read this text and answer question: {question}:\\n{context}\")\n",
    "print(resp.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks discussed in the text:\n",
      "1. Sequence Classification Tasks: Involves feeding the same input into the encoder and decoder, and using the final hidden state of the final decoder token for classification.\n",
      "2. Token Classification Tasks: Involves feeding the complete document into the encoder and decoder, using the top hidden state of the decoder as a representation for each word for classification.\n",
      "3. Sequence Generation Tasks: Involves fine-tuning BART for tasks like abstractive question answering and summarization, where the encoder input is the input sequence and the decoder generates outputs autoregressively.\n",
      "4. Machine Translation: Involves using the entire BART model as a single pretrained decoder for machine translation by adding a new set of encoder parameters learned from bitext.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:19.895854Z",
     "start_time": "2024-06-06T13:49:19.892668Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1-NOhTfkF3q",
    "outputId": "ad4fa612-1493-4c10-96e8-700587a8609f",
    "ExecuteTime": {
     "end_time": "2024-06-06T13:49:19.914617Z",
     "start_time": "2024-06-06T13:49:19.897615Z"
    }
   },
   "source": [
    "# from llama_index.llms import OpenAI\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "context = selected_section.to_html(include_children=True, recurse=True)\n",
    "# question = \"\"\n",
    "\n",
    "resp = openai.Completion.create(\n",
    "            engine=\"davinci\",\n",
    "            prompt=(f\"Please summarize the following text:\\n{context}\\n\\nSummary:\"),\n",
    "            temperature=0.5,\n",
    "            max_tokens=1024,\n",
    "            n = 1,\n",
    "            stop=None\n",
    "        )\n",
    "resp"
   ],
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAPIRemovedInV1\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m context \u001B[38;5;241m=\u001B[39m selected_section\u001B[38;5;241m.\u001B[39mto_html(include_children\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, recurse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# question = \"\"\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m            \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdavinci\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPlease summarize the following text:\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcontext\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mSummary:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m resp\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/openai/lib/_old_api.py:39\u001B[0m, in \u001B[0;36mAPIRemovedInV1Proxy.__call__\u001B[0;34m(self, *_args, **_kwargs)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m_args: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m APIRemovedInV1(symbol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_symbol)\n",
      "\u001B[0;31mAPIRemovedInV1\u001B[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1-NOhTfkF3q",
    "outputId": "ad4fa612-1493-4c10-96e8-700587a8609f"
   },
   "source": [
    "# resp = OpenAI().complete(f\"read this text and answer question: {question}:\\n{context}\")\n",
    "print(resp.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs6WO91KlOkF"
   },
   "source": [
    "**Analyze a Table using prompts**\n",
    "\n",
    "With LayoutPDFReader, you can iterate through all the tables in a document and use the power of LLMs to analyze a Table Let's look at the 6th table in this document. If you are using a notebook, you can display the table as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "Wdqvkig-lX6g",
    "outputId": "1d1ab688-10d0-43eb-e96f-a021f7fe8e5f"
   },
   "source": [
    "from IPython.core.display import display, HTML\n",
    "HTML(doc.tables()[5].to_html())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NEl4vzvlg5W"
   },
   "source": [
    "Now let's ask a question to analyze this table:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7oHRsbcloL6",
    "outputId": "c5896296-93e7-49af-94b3-a23ea4f939d3"
   },
   "source": [
    "# from llama_index.llms import OpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "context = doc.tables()[5].to_html()\n",
    "resp = OpenAI().complete(f\"read this table and answer question: which model has the best performance on squad 2.0:\\n{context}\")\n",
    "print(resp.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFefOzQ9ltM3"
   },
   "source": [
    "That's it! LayoutPDFReader also supports tables with nested headers and header rows.\n",
    "\n",
    "Here's an example with nested headers (note that the HTML doesn't render properly in ipython but the html structure is correct):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "SShVijKPlz36",
    "outputId": "26077bba-a90a-4971-9b00-b3027acfeb6b"
   },
   "source": [
    "from IPython.core.display import display, HTML\n",
    "HTML(doc.tables()[6].to_html())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F0GF1qNmJlS"
   },
   "source": [
    "Now let's ask an interesting question:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsUCGpm3mRJJ",
    "outputId": "de33d4ed-0478-47ef-d24e-cbb738e403f5"
   },
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "context = doc.tables()[6].to_html()\n",
    "question = \"tell me about R1 of bart for different datasets\"\n",
    "resp = OpenAI().complete(f\"read this table and answer question: {question}:\\n{context}\")\n",
    "print(resp.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE7I3L7XmWl3"
   },
   "source": [
    "\n",
    "**Vector search and Retrieval Augmented Generation with Smart Chunking**\n",
    "\n",
    "LayoutPDFReader does smart chunking keeping the integrity of related text together:\n",
    "\n",
    "All list items are together including the paragraph that precedes the list.\n",
    "Items in a table are chuncked together\n",
    "Contextual information from section headers and nested section headers is included\n",
    "The following code creates a LlamaIndex query engine from LayoutPDFReader document chunks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YGqsLP5zmirK"
   },
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex([])\n",
    "for chunk in doc.chunks():\n",
    "    index.insert(Document(text=chunk.to_context_text(), extra_info={}))\n",
    "query_engine = index.as_query_engine()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f797GR6nGKw"
   },
   "source": [
    "Let's run one query:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vC9H9c38nOsG",
    "outputId": "f0366196-1338-4a83-e54b-d676d29a2477"
   },
   "source": [
    "response = query_engine.query(\"list all the tasks that work with bart\")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ylioaL0nUyr"
   },
   "source": [
    "Let's try another query that needs answer from a table:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZEHBSsnnc30",
    "outputId": "c96cf16d-2ec9-41b3-ea25-060c7e35895c"
   },
   "source": [
    "response = query_engine.query(\"what is the bart performance score on squad\")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT67xijEnh1X"
   },
   "source": [
    "**Get the Raw JSON**\n",
    "\n",
    "To get the complete json returned by llmsherpa service and process it differently, simply get the json attribute"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YHwKINfnri7",
    "outputId": "ae1e8fd7-4715-4b6b-ccb9-1df9fdee68df"
   },
   "source": [
    "doc.json"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VGuCGzaEdb0J"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
