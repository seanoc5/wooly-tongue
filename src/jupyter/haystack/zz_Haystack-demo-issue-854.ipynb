{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3e8dca-257a-449b-9f01-95a96d651252",
   "metadata": {},
   "source": [
    "# Haystack Semantic Search demo\n",
    "https://github.com/deepset-ai/haystack/issues/854  \n",
    "in progress (haystack env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19fa35e-5c80-42f9-847e-e04b54624b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chroma-haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73a7d44-0f22-43a0-bd46-03e366ab080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack.document_store.faiss import FAISSDocumentStore\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d727ce1f-7da3-4fbf-bd5a-a95b828397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma is used in-memory so we use the same instances in the two pipelines below\n",
    "document_store = ChromaDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd062f7e-5ef7-441f-b8f5-c12d6218065e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clean_wiki_text' from 'haystack.components.preprocessors.document_cleaner' (/home/sean/miniforge3/envs/haystack/lib/python3.10/site-packages/haystack/components/preprocessors/document_cleaner.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_cleaner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_wiki_text\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_files_to_dicts, fetch_archive_from_http\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhaystack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DensePassageRetriever\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'clean_wiki_text' from 'haystack.components.preprocessors.document_cleaner' (/home/sean/miniforge3/envs/haystack/lib/python3.10/site-packages/haystack/components/preprocessors/document_cleaner.py)"
     ]
    }
   ],
   "source": [
    "from haystack.components.preprocessors.document_cleaner import clean_wiki_text\n",
    "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
    "from haystack.retriever.dense import DensePassageRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e62b92-a856-45a6-ba23-40aa32399a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
    "\n",
    "# ## Preprocessing of documents\n",
    "# Let's first get some documents that we want to query\n",
    "doc_dir = \"data/article_txt_got\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
    "\n",
    "# convert files to dicts containing documents that can be indexed to our datastore\n",
    "dicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n",
    "\n",
    "# Now, let's write the docs to our DB.\n",
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644bc87-2498-4071-8b44-773f7e4974b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retriever\n",
    "\n",
    "# Recommended: DPR\n",
    "# retriever = DensePassageRetriever(document_store=document_store,\n",
    "#                                   query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "#                                   passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "#                                   max_seq_len_query=64,\n",
    "#                                   max_seq_len_passage=256,\n",
    "#                                   batch_size=2,\n",
    "#                                   use_gpu=True,\n",
    "#                                   embed_title=True,\n",
    "#                                   use_fast_tokenizers=True\n",
    "#                                   )\n",
    "\n",
    "# Alternative: Single encoder for example via sentence transformers\n",
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "retriever = EmbeddingRetriever(document_store=document_store,\n",
    "                               embedding_model=\"sentence-transformers/roberta-base-nli-stsb-mean-tokens\", # from huggingface's model hub\n",
    "                               use_gpu=True,\n",
    "                               model_format=\"farm\", # you can also use \"sentence-transformers\" here to load the models with the respective framework\n",
    "                               )\n",
    "\n",
    "\n",
    "# Important:\n",
    "# Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all\n",
    "# previously indexed documents and update their embedding representation.\n",
    "# While this can be a time consuming operation (depending on corpus size), it only needs to be done once.\n",
    "# At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.\n",
    "document_store.update_embeddings(retriever)\n",
    "\n",
    "### Pipeline\n",
    "# Select the default pipeline for document search or build your own custom one (e.g. combining multiple retrievers).\n",
    "# See details here: https://haystack.deepset.ai/docs/latest/pipelinesmd\n",
    "from haystack.pipeline import DocumentSearchPipeline\n",
    "pipe = DocumentSearchPipeline(retriever=retriever)\n",
    "\n",
    "## Voil√†! Ask a question!\n",
    "prediction = pipe.run(query=\"Who is the father of Arya Stark?\", top_k_retriever=10)\n",
    "print(prediction)\n",
    "# Returns list of docs:\n",
    "# {'query': 'Who is the father of Arya Stark?', 'documents': [\n",
    "#{'text': '\\n===Storylines===\\nBrandon \"Bran\" Stark is the second son and fourth child of Eddard and Catelyn Stark. He was named after his deceased uncle, Brandon.', 'id': 'e3fc0543-52af-40f7-a4b0-5e68e350f543', 'score': 218.03285, 'probability': 0.8984690445729299, 'question': None, 'meta': {'vector_id': '2220', 'name': '331_Bran_Stark.txt'}, 'embedding': None}, \n",
    "#{'text': '\\n=== Robb Stark ===\\nRobb Stark is the oldest child of Eddard and Catelyn Stark, and the heir to Winterfell. He is not a POV character, but features in the POV chapters of his family members in the first three novels in the series.\\nIn the HBO television adaptation, he is portrayed by Richard Madden.', 'id': 'af4010af-6257-46f3-9331-8b0c3fb89298', 'score': 179.93167, 'probability': 0.8580657384755829, 'question': None, 'meta': {'vector_id': '1711', 'name': '30_List_of_A_Song_of_Ice_and_Fire_characters.txt'}, 'embedding': None}, \n",
    "#...]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "haystack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
