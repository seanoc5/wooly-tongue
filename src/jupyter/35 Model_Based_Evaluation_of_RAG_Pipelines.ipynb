{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813d7c18-b849-4649-8a8e-8a2b0260207a",
   "metadata": {},
   "source": [
    "# 35 - Model_Based_Evaluation_of_RAG_Pipelines\n",
    "https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/35_Model_Based_Evaluation_of_RAG_Pipelines.ipynb\n",
    "\n",
    "working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dc3509-a863-4570-89e2-28f52823a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab51651-9b76-4291-97f3-da33f00b2178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
    "# or from: https://docs.haystack.deepset.ai/docs/huggingfacelocalgenerator\n",
    "# docstore.write_documents([Document(content=\"Rome is the capital of Italy\"), Document(content=\"Paris is the capital of France\")])\n",
    "\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3a6e73-1dcc-4441-bfda-5ede3ba6e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from getpass import getpass\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "\n",
    "retriever = InMemoryBM25Retriever(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093cc2a7-2367-4382-8f7c-e9933350b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.generators import HuggingFaceLocalGenerator\n",
    "\n",
    "generator = HuggingFaceLocalGenerator(model=\"google/flan-t5-large\",\n",
    "                                      task=\"text2text-generation\",\n",
    "                                      generation_kwargs={\n",
    "                                        \"max_new_tokens\": 100,\n",
    "                                        \"temperature\": 0.9,\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0119231d-7f87-4a2e-9a55-9c9087713972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'replies': ['john wayne']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/miniforge3/envs/haystack-test/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generator.warm_up()\n",
    "print(generator.run(\"Who is the best American actor?\"))\n",
    "# {'replies': ['john wayne']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b920d17d-2397-4d38-9f5d-d62a58ccdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"How many wonders are there?\"\n",
    "query = \"What statue is a wonder?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8223d5dc-bcf9-474a-8ec0-7d4dd7c9f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{query}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5add9268-27f9-483c-acb7-8e1bed8ce299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7f39d81cfd40>\n",
       "ðŸš… Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: HuggingFaceLocalGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "pipe = Pipeline()\n",
    "\n",
    "# pipe.add_component(\"retriever\", InMemoryBM25Retriever(document_store=docstore))\n",
    "# pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "\n",
    "pipe.add_component(\"retriever\", retriever)\n",
    "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
    "\n",
    "pipe.add_component(\"llm\", generator)\n",
    "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "pipe.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739d4a1b-8a4e-43f2-891c-e5e7ae8d59e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d285edb6c0f44ca95062d60eb8be5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking by BM25...:   0%|          | 0/151 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3210 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm': {'replies': ['The Statue of Zeus at Olympia']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res=pipe.run({\n",
    "    \"prompt_builder\": {\n",
    "        \"query\": query\n",
    "    },\n",
    "    \"retriever\": {\n",
    "        \"query\": query\n",
    "    }\n",
    "})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede7d35-7699-4be5-b203-8c98b81fe323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
