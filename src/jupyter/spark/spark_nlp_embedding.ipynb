{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "svJ-JDYK3aAm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7af93c62-7dc3-4cf0-bab9-f897efea5f23",
    "ExecuteTime": {
     "end_time": "2024-09-24T17:59:52.465186Z",
     "start_time": "2024-09-24T17:59:52.462990Z"
    }
   },
   "source": "# !pip install -q pyspark==3.4.1 spark-nlp==5.3.2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip uninstall -y pyspark spark-nlp"
   ],
   "metadata": {
    "id": "8go9haJELZ3w",
    "ExecuteTime": {
     "end_time": "2024-09-24T17:59:52.519553Z",
     "start_time": "2024-09-24T17:59:52.515972Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:59:52.571033Z",
     "start_time": "2024-09-24T17:59:52.567770Z"
    }
   },
   "source": [
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "from sparknlp.annotator import BertEmbeddings, Tokenizer\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "# Flag to control GPU monitoring\n",
    "gpu_monitoring_active = True\n",
    "gpu_utilization_data = []"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:59:52.618744Z",
     "start_time": "2024-09-24T17:59:52.615269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Function to monitor GPU utilization in parallel\n",
    "def monitor_gpu_utilization():\n",
    "    while gpu_monitoring_active:  # Check if the flag is active\n",
    "        gpu_stats = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "        utilization_match = re.search(r'(\\d+)%\\s+Default', gpu_stats)\n",
    "        if utilization_match:\n",
    "            gpu_utilization = int(utilization_match.group(1))\n",
    "            gpu_utilization_data.append(gpu_utilization)\n",
    "\n",
    "        # Check the flag every second within a longer sleep to allow for early exit\n",
    "        for _ in range(1):  # Total of 5 seconds (adjust based on your need)\n",
    "            if not gpu_monitoring_active:\n",
    "                return\n",
    "            time.sleep(1)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:59:52.666659Z",
     "start_time": "2024-09-24T17:59:52.664032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_plot(frame):\n",
    "    plt.cla()  # Clear the previous plot\n",
    "    plt.plot(gpu_utilization_data, label='GPU Utilization (%)')\n",
    "    plt.ylim(0, 100)  # GPU utilization percentage is between 0 and 100\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('GPU Utilization (%)')\n",
    "    plt.title('Real-time GPU Utilization')\n",
    "    plt.legend(loc='upper right')"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "id": "kAPG7eEv5YYh",
    "ExecuteTime": {
     "end_time": "2024-09-24T17:59:52.715142Z",
     "start_time": "2024-09-24T17:59:52.711761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start GPU monitoring in a separate thread\n",
    "gpu_thread = threading.Thread(target=monitor_gpu_utilization)\n",
    "gpu_thread.daemon = True  # This allows the thread to exit when the main program finishes\n",
    "gpu_thread.start()\n",
    "\n",
    "# Initialize the Spark session with GPU support\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Spark NLP with GPU\") \\\n",
    "#     .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:5.3.2\") \\\n",
    "#     .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "#     .config(\"spark.task.resource.gpu.amount\", \"1\") \\\n",
    "#     .config(\"spark.driver.memory\", \"16G\") \\\n",
    "#     .config(\"spark.executor.memory\", \"16G\") \\\n",
    "#     .config(\"spark.executor.cores\", \"4\") \\\n",
    "#     .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "#     .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"2048\") \\\n",
    "#     .config(\"spark.driver.extraJavaOptions\", \"-Dtensorflow.gpu=true\") \\\n",
    "#     .getOrCreate()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "spark = sparknlp.start(gpu=True)\n",
    "# Start time tracking\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate a large dataset to fully utilize the GPU\n",
    "# Create a large dataset of strings (not arrays of strings)\n",
    "text_data = [\"This is a sample sentence for embedding.\"] * 100000  # Repeat sentence to create a large dataset\n",
    "\n",
    "# Create a DataFrame with single strings (not arrays)\n",
    "data_df = spark.createDataFrame([Row(text=row) for row in text_data])\n",
    "data_df.show()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:59:54.310861Z",
     "start_time": "2024-09-24T17:59:52.760581Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/24 13:59:53 WARN Utils: Your hostname, minti9 resolves to a loopback address: 127.0.1.1; using 192.168.1.101 instead (on interface enp4s0)\n",
      "24/09/24 13:59:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/sean/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sean/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp-gpu_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f5c23141-ce5c-443d-bf2c-41b4c1605491;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-gpu_2.12;5.4.1 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/sean/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound commons-codec#commons-codec;1.15 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in local-m2-cache\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in local-m2-cache\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in local-m2-cache\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in local-m2-cache\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in local-m2-cache\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in local-m2-cache\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in local-m2-cache\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in local-m2-cache\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in local-m2-cache\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in local-m2-cache\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in local-m2-cache\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-gpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.18.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-gpu_2.12;0.1.0 in central\n",
      ":: resolution report :: resolve 487ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from local-m2-cache in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from local-m2-cache in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from local-m2-cache in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from local-m2-cache in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from local-m2-cache in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from local-m2-cache in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from local-m2-cache in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-gpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-gpu_2.12;5.4.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-gpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.18.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from local-m2-cache in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from local-m2-cache in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from local-m2-cache in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   84  |   1   |   1   |   5   ||   79  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\t\t[NOT FOUND  ] com.google.guava#guava;31.1-jre!guava.jar(bundle) (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/sean/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] javax.annotation#javax.annotation-api;1.3.2!javax.annotation-api.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/sean/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t::              FAILED DOWNLOADS            ::\n",
      "\n",
      "\t\t:: ^ see resolution messages for details  ^ ::\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t:: com.google.guava#guava;31.1-jre!guava.jar(bundle)\n",
      "\n",
      "\t\t:: javax.annotation#javax.annotation-api;1.3.2!javax.annotation-api.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      "Exception in thread \"main\" java.lang.RuntimeException: [download failed: com.google.guava#guava;31.1-jre!guava.jar(bundle), download failed: javax.annotation#javax.annotation-api;1.3.2!javax.annotation-api.jar]\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1608)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPySparkRuntimeError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43msparknlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgpu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Start time tracking\u001B[39;00m\n\u001B[1;32m      3\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/sparknlp/__init__.py:301\u001B[0m, in \u001B[0;36mstart\u001B[0;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001B[0m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkRealTimeOutput()\u001B[38;5;241m.\u001B[39mspark_session\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 301\u001B[0m     spark_session \u001B[38;5;241m=\u001B[39m \u001B[43mstart_without_realtime_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m spark_session\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/sparknlp/__init__.py:199\u001B[0m, in \u001B[0;36mstart.<locals>.start_without_realtime_output\u001B[0;34m()\u001B[0m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m         builder\u001B[38;5;241m.\u001B[39mconfig(key, value)\n\u001B[0;32m--> 199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    495\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[0;32m--> 497\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[1;32m    500\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/context.py:515\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 515\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/context.py:201\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    197\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m     )\n\u001B[0;32m--> 201\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    204\u001B[0m         master,\n\u001B[1;32m    205\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    215\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    216\u001B[0m     )\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/context.py:436\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[0;32m--> 436\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    437\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[0;32m~/miniforge3/envs/wt2/lib/python3.10/site-packages/pyspark/java_gateway.py:107\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[0;34m(conf, popen_kwargs)\u001B[0m\n\u001B[1;32m    104\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[1;32m    108\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJAVA_GATEWAY_EXITED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    109\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    110\u001B[0m     )\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[1;32m    113\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[0;31mPySparkRuntimeError\u001B[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "L53-wtiKKvKB"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Define a document assembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Step 3: Load a larger, more complex pretrained model to increase GPU usage\n",
    "bert_embeddings = BertEmbeddings.pretrained(\"bert_large_uncased\", \"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setBatchSize(1)  # Increase batch size to use more GPU resources\n",
    "\n",
    "# Step 4: Create a pipeline with document assembler, tokenizer, and embeddings\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, bert_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig, update_plot, interval=1000)\n",
    "def run_pipeline():\n",
    "    model = pipeline.fit(data_df)\n",
    "    embedded_df = model.transform(data_df)\n",
    "    # Show the embeddings (only the first 2 rows to avoid long output)\n",
    "    embedded_df.select(\"embeddings.result\").show(2, truncate=False)\n",
    "\n",
    "pipeline_thread = threading.Thread(target=run_pipeline)\n",
    "pipeline_thread.start()\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "5RjqAQ9dK3Ib"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pipeline_thread.join()\n",
    "# End time tracking\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total time\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Stop GPU monitoring\n",
    "gpu_monitoring_active = False\n",
    "\n",
    "# Wait for the GPU monitoring thread to finish\n",
    "gpu_thread.join()\n",
    "print(\"GPU monitoring stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwv0v3ws0xr0"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "toc_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
